# Privacy Policy Crawler

## Overview

This project is a crawler written in Scrapy that takes a file with URLs and saves the HTMLs distributed in 1000 folders. The crawler works at a depth of 2 and creates success and failure URL files in addition to the log file. A hash based on the URL is created and used as the name of the file to be saved. 


